use anyhow::Result;
use serde::{Deserialize, Serialize};
use serde_json::json;
use chrono::Utc;
use rbatis::RBatis;
use crate::behavior_analytics::BehaviorAnalytics;
use super::r#trait::AIService;
use super::common::{
    AIGeneratedAchievement, AIGeneratedTask, AIGeneratedTaskPlan, ExpertMatch, Expert,
    format_ai_output, get_expert_database, build_achievement_prompt_from_summary,
    validate_generated_achievement, validate_generated_task,
    AITaskPrimaryFields, AITaskSecondaryFields, AIPlanPrimaryFields, AIPlanSecondaryFields
};

// OpenAI API è«‹æ±‚çµæ§‹
#[derive(Serialize)]
struct OpenAIRequest {
    model: String,
    messages: Vec<ChatMessage>,
    max_completion_tokens: i32,
    response_format: ResponseFormat,
}

#[derive(Serialize)]
struct ResponseFormat {
    #[serde(rename = "type")]
    format_type: String,
}

#[derive(Serialize, Deserialize)]
struct ChatMessage {
    role: String,
    content: String,
}

// OpenAI API å›æ‡‰çµæ§‹
#[derive(Deserialize)]
struct OpenAIResponse {
    choices: Vec<Choice>,
}

#[derive(Deserialize)]
struct Choice {
    message: ChatMessage,
}


pub struct OpenAIService {
    api_key: String,
    model: String,
    client: reqwest::Client,
}

impl OpenAIService {
    pub fn new(api_key: String, model: String) -> Self {
        Self {
            api_key,
            model,
            client: reqwest::Client::new(),
        }
    }
}

#[async_trait::async_trait]
impl AIService for OpenAIService {
    async fn generate_achievement_from_text(&self, user_input: &str) -> Result<AIGeneratedAchievement> {
        let system_prompt = r#"ä½ æ˜¯ä¸€å€‹æˆå°±è¨­è¨ˆåŠ©æ‰‹ã€‚æ ¹æ“šç”¨æˆ¶çš„è¡Œç‚ºæ•¸æ“šåˆ†æï¼Œç”Ÿæˆå€‹æ€§åŒ–ä¸”å…·æœ‰æ¿€å‹µæ€§çš„æˆå°±ã€‚

è«‹ä»”ç´°åˆ†æç”¨æˆ¶çš„ï¼š
1. å·²æœ‰æˆå°±åˆ—è¡¨
2. ä»»å‹™å®Œæˆç‹€æ³
3. ä»»å‹™å–æ¶ˆ/å¤±æ•—ç‹€æ³
4. å¾…å®Œæˆä»»å‹™

**è¨­è¨ˆåŸå‰‡ï¼š**
- æˆå°±åç¨±è¦å¹½é»˜ä¸”å…·é«”ï¼Œå¦‚ã€Œæˆç‚ºè‹±èªå­—å…¸ã€ã€Œè·‘ç«å…¥é­”ã€
- åŸºæ–¼ç”¨æˆ¶å¯¦éš›è¡Œç‚ºæ¨¡å¼ç”Ÿæˆï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
- å¦‚æœç”¨æˆ¶åœ¨æŸé ˜åŸŸå·²æœ‰åŸºç¤æˆå°±ä¸”è¡¨ç¾å„ªç§€ï¼Œå¯è€ƒæ…®å‡ç´šç‰ˆæˆå°±
- é¿å…èˆ‡ç¾æœ‰æˆå°±é‡è¤‡

**æˆå°±åˆ†é¡ï¼š**
- task_mastery: ä»»å‹™ç²¾é€šé¡
- consistency: æŒçºŒæ€§é¡
- challenge_overcome: å…‹æœæŒ‘æˆ°é¡
- skill_development: æŠ€èƒ½ç™¼å±•é¡

**é”æˆæ¢ä»¶é¡å‹ï¼š**
- consecutive_days: é€£çºŒå¤©æ•¸
- total_completions: ç¸½å®Œæˆæ¬¡æ•¸
- task_complete: å®Œæˆä»»å‹™ç¸½æ•¸
- streak_recovery: å¾å¤±æ•—ä¸­æ¢å¾©
- skill_level: æŠ€èƒ½ç­‰ç´š
- learning_task_complete: å­¸ç¿’ä»»å‹™å®Œæˆ
- intelligence_attribute: æ™ºåŠ›å±¬æ€§é”æˆ
- endurance_attribute: æ¯…åŠ›å±¬æ€§é”æˆ
- creativity_attribute: å‰µé€ åŠ›å±¬æ€§é”æˆ
- social_attribute: ç¤¾äº¤åŠ›å±¬æ€§é”æˆ
- focus_attribute: å°ˆæ³¨åŠ›å±¬æ€§é”æˆ
- adaptability_attribute: é©æ‡‰åŠ›å±¬æ€§é”æˆ

**ç¶“é©—å€¼çå‹µè¨ˆç®—ï¼š**
- åŸºæ–¼é›£åº¦ï¼šç°¡å–®æˆå°± 50-100ï¼Œä¸­ç­‰ 100-200ï¼Œå›°é›£ 200-500

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{
  "name": "æˆå°±åç¨±ï¼ˆå¹½é»˜ä¸”å…·é«”ï¼‰",
  "description": "æˆå°±æè¿°ï¼ˆé¸å¡«ï¼‰",
  "icon": "åœ–æ¨™åç¨±ï¼ˆé¸å¡«ï¼‰",
  "category": "æˆå°±åˆ†é¡",
  "requirement_type": "é”æˆæ¢ä»¶é¡å‹",
  "requirement_value": æ•¸å€¼,
  "experience_reward": ç¶“é©—å€¼çå‹µ
}

ç¯„ä¾‹ï¼š
è¼¸å…¥ï¼šä½¿ç”¨è€…é€£çºŒå®Œæˆã€ŒèƒŒè‹±èªå–®å­—ã€30å¤©ï¼Œä½†ç¶“å¸¸å–æ¶ˆã€Œé‹å‹•ã€ä»»å‹™
è¼¸å‡ºï¼š
{
  "name": "æˆç‚ºè‹±èªå­—å…¸",
  "description": "é€£çºŒ30å¤©å®ŒæˆèƒŒè‹±èªå–®å­—ï¼Œè©å½™é‡å·²ç¶“è¶…è¶Šä¸€èˆ¬å­—å…¸",
  "icon": "ğŸ“–",
  "category": "task_mastery",
  "requirement_type": "consecutive_days",
  "requirement_value": 30,
  "experience_reward": 300
}"#;

        let user_message = format!("è«‹æ ¹æ“šä»¥ä¸‹ç”¨æˆ¶è¡Œç‚ºæ•¸æ“šç”Ÿæˆåˆé©çš„æˆå°±ï¼š{}", user_input);

        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: system_prompt.to_string(),
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: user_message.clone(),
                },
            ],
            max_completion_tokens: 4000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!("[AI INPUT][generate_achievement_from_text] {}", format_ai_output(&body));
        }

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!("[AI OUTPUT][generate_achievement_from_text] {}", format_ai_output(&response_text));

        if !status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ ({}): {}", status, response_text));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;

        if let Some(choice) = openai_response.choices.first() {
            let achievement_json = &choice.message.content;
            let generated_achievement: AIGeneratedAchievement = serde_json::from_str(achievement_json)?;

            validate_generated_achievement(&generated_achievement)?;

            Ok(generated_achievement)
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }

    async fn generate_achievement_from_user_id(&self, rb: &RBatis, user_id: &str) -> Result<AIGeneratedAchievement> {
        // 1. ç”Ÿæˆç”¨æˆ·è¡Œä¸ºæ‘˜è¦
        log::info!("ä¸ºç”¨æˆ· {} ç”Ÿæˆè¡Œä¸ºæ‘˜è¦...", user_id);
        let summary = BehaviorAnalytics::generate_summary(rb, user_id).await?;
        log::info!("è¡Œä¸ºæ‘˜è¦ç”Ÿæˆå®Œæˆï¼šå®Œæˆ{}ä¸ªä»»åŠ¡ï¼Œæœ€é•¿è¿ç»­{}å¤©", summary.total_tasks_completed, summary.longest_streak.days);

        // 2. æ„å»ºåŸºäºæ‘˜è¦çš„ prompt
        let system_prompt = build_achievement_prompt_from_summary(&summary);

        // 3. è°ƒç”¨ AI ç”Ÿæˆæˆå°±
        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: system_prompt,
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: "è«‹åŸºæ–¼ä»¥ä¸Šç”¨æˆ¶æ•¸æ“šï¼Œç”Ÿæˆä¸€å€‹æœ€åˆé©çš„æˆå°±ã€‚".to_string(),
                },
            ],
            max_completion_tokens: 4000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!("[AI INPUT][generate_achievement_from_user_id] {}", format_ai_output(&body));
        }

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!("[AI OUTPUT][generate_achievement_from_user_id] {}", format_ai_output(&response_text));

        if !status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ ({}): {}", status, response_text));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;

        if let Some(choice) = openai_response.choices.first() {
            let achievement_json = &choice.message.content;
            let generated_achievement: AIGeneratedAchievement = serde_json::from_str(achievement_json)?;

            // é©—è­‰ç”Ÿæˆçš„æˆå°±
            validate_generated_achievement(&generated_achievement)?;

            Ok(generated_achievement)
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }

    async fn generate_task_preview(&self, prompt: &str) -> Result<String> {
        let request = serde_json::json!({
            "model": self.model.clone(),
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€å€‹å……æ»¿æ´»åŠ›å’Œé¼“å‹µçš„ä»»å‹™åŠ©æ‰‹ã€‚ç”¨ç©æ¥µæ­£é¢çš„èªæ°£ç‚ºç”¨æˆ¶ä»‹ç´¹ä»»å‹™ï¼Œè®“ä»–å€‘æ„Ÿåˆ°èˆˆå¥®å’Œæœ‰å‹•åŠ›å»å®Œæˆã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_completion_tokens": 4000
        });

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!("[AI INPUT][generate_task_preview] {}", format_ai_output(&body));
        }

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!("[AI OUTPUT][generate_task_preview] {}", format_ai_output(&response_text));

        if !status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ ({}): {}", status, response_text));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;

        if let Some(choice) = openai_response.choices.first() {
            Ok(choice.message.content.clone())
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }

    async fn generate_task_preview_with_history(&self, system_prompt: &str, history: &[(String, String)], current_message: &str) -> Result<String> {
        let mut messages = vec![];

        // å…ˆæ·»åŠ æ­·å²å°è©±
        for (user_msg, assistant_msg) in history {
            messages.push(serde_json::json!({
                "role": "user",
                "content": user_msg
            }));
            messages.push(serde_json::json!({
                "role": "assistant",
                "content": assistant_msg
            }));
        }

        // ç„¶å¾Œæ·»åŠ ç³»çµ±æç¤ºè©
        messages.push(serde_json::json!({
            "role": "system",
            "content": system_prompt
        }));

        // æœ€å¾Œæ·»åŠ ç•¶å‰ç”¨æˆ¶è¨Šæ¯
        messages.push(serde_json::json!({
            "role": "user",
            "content": current_message
        }));

        let request = serde_json::json!({
            "model": self.model.clone(),
            "messages": messages,
            "max_completion_tokens": 4000
        });

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!("[AI INPUT][generate_task_preview_with_history] {}", format_ai_output(&body));
        }

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!("[AI OUTPUT][generate_task_preview_with_history] {}", format_ai_output(&response_text));

        if !status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ ({}): {}", status, response_text));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;

        if let Some(choice) = openai_response.choices.first() {
            Ok(choice.message.content.clone())
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }

    async fn generate_task_from_text(&self, user_input: &str) -> Result<AIGeneratedTask> {
        let now = Utc::now();
        let current_time_str = now.format("%Y-%m-%dT%H:%M:%S").to_string();

        let primary_prompt = format!(
            r#"ä½ æ˜¯ä¸€å€‹ä»»å‹™è¦åŠƒåŠ©æ‰‹ã€‚æ ¹æ“šç”¨æˆ¶çš„è‡ªç„¶èªè¨€æè¿°ï¼Œå…ˆç”Ÿæˆä»»å‹™çš„ä¸»è¦æ¬„ä½ã€‚

**é‡è¦ï¼šç¾åœ¨çš„æ™‚é–“æ˜¯ {}ã€‚** åœ¨ç”Ÿæˆä»»ä½•èˆ‡æ—¥æœŸç›¸é—œçš„æ¬„ä½ï¼ˆå¦‚ due_dateï¼‰æ™‚ï¼Œè«‹ä»¥æ­¤æ™‚é–“ç‚ºåŸºæº–é€²è¡Œæ¨ç®—ã€‚

**æˆªæ­¢æ—¥æœŸç”Ÿæˆè¦å‰‡ï¼š**
- å°æ–¼å¤§éƒ¨åˆ†ä»»å‹™ï¼Œä½ éƒ½æ‡‰è©²è¨­å®šä¸€å€‹åˆç†çš„æˆªæ­¢æ—¥æœŸ
- çŸ­æœŸä»»å‹™ï¼ˆ1-3å¤©å…§å®Œæˆï¼‰ï¼šè¨­å®š1-3å¤©å¾Œçš„æˆªæ­¢æ—¥æœŸ
- ä¸­æœŸä»»å‹™ï¼ˆ1-2é€±å®Œæˆï¼‰ï¼šè¨­å®š1-2é€±å¾Œçš„æˆªæ­¢æ—¥æœŸ
- é•·æœŸä»»å‹™ï¼ˆ1å€‹æœˆä»¥ä¸Šï¼‰ï¼šè¨­å®š1-3å€‹æœˆå¾Œçš„æˆªæ­¢æ—¥æœŸ
- åªæœ‰å°æ–¼æ²’æœ‰æ˜ç¢ºæ™‚é–“é™åˆ¶çš„ç¿’æ…£é¡ä»»å‹™æ‰è¨­å®š due_date ç‚º null
- å¦‚æœç”¨æˆ¶æ˜ç¢ºæåˆ°æ™‚é–“ï¼ˆå¦‚"æ˜å¤©"ã€"ä¸‹é€±"ã€"æœˆåº•"ï¼‰ï¼Œä¸€å®šè¦æ ¹æ“šç•¶å‰æ™‚é–“è¨ˆç®—å°æ‡‰çš„æˆªæ­¢æ—¥æœŸ

ä»»å‹™é¡å‹èªªæ˜ï¼š
- main: ä¸»è¦ä»»å‹™ï¼ˆé‡è¦çš„é•·æœŸç›®æ¨™ï¼Œé€šå¸¸è¨­å®šè¼ƒé•·çš„æˆªæ­¢æ—¥æœŸï¼‰
- side: å‰¯ç·šä»»å‹™ï¼ˆæ¬¡è¦çš„çŸ­æœŸä»»å‹™ï¼Œé€šå¸¸è¨­å®šè¼ƒçŸ­çš„æˆªæ­¢æ—¥æœŸï¼‰
- challenge: æŒ‘æˆ°ä»»å‹™ï¼ˆå›°é›£ä¸”æœ‰æˆå°±æ„Ÿçš„ä»»å‹™ï¼Œæ ¹æ“šå…·é«”å…§å®¹è¨­å®šæˆªæ­¢æ—¥æœŸï¼‰
- daily: æ—¥å¸¸ä»»å‹™ï¼ˆä¾‹è¡Œæ€§ä»»å‹™ï¼Œé‡è¤‡æ€§ä»»å‹™é€šå¸¸ä¸è¨­å®šæˆªæ­¢æ—¥æœŸï¼‰

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{{
  "title": "ä»»å‹™æ¨™é¡Œ",
  "description": "ä»»å‹™æè¿°ï¼ˆé¸å¡«ï¼‰",
  "task_type": "main/side/challenge/daily",
  "due_date": "æˆªæ­¢æ—¥æœŸï¼ˆISO 8601æ ¼å¼ï¼Œå¤§å¤šæ•¸æƒ…æ³ä¸‹éƒ½æ‡‰è©²è¨­å®šï¼Œè‹¥ç‚ºé‡è¤‡æ€§ä»»å‹™å‰‡ç‚º nullï¼‰",
  "recurrence_pattern": "é‡è¤‡æ¨¡å¼ï¼ˆåƒ…åœ¨é‡è¤‡æ€§ä»»å‹™æ™‚å¡«å¯«ï¼Œå¦å‰‡ç‚º nullï¼‰"
}}

è‹¥åˆ¤å®šç‚ºé‡è¤‡æ€§ä»»å‹™ï¼Œrecurrence_pattern å¿…é ˆæ˜¯ "daily"ã€"weekdays"ã€"weekends" æˆ– "weekly"ï¼Œä¸” due_date å¿…é ˆç‚º nullã€‚
"#,
            current_time_str
        );

        let primary_request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: primary_prompt,
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: format!("è«‹æ ¹æ“šä»¥ä¸‹æè¿°ç”Ÿæˆä»»å‹™ä¸»è¦æ¬„ä½ï¼š{}", user_input),
                },
            ],
            max_completion_tokens: 2000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&primary_request) {
            log::info!("[AI INPUT][generate_task_from_text_primary] {}", format_ai_output(&body));
        }

        let primary_response = self
            .client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&primary_request)
            .send()
            .await?;

        let primary_status = primary_response.status();
        let primary_text = primary_response.text().await?;
        log::info!("[AI OUTPUT][generate_task_from_text_primary] {}", format_ai_output(&primary_text));

        if !primary_status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ (primary) ({}): {}", primary_status, primary_text));
        }

        let primary_parsed: OpenAIResponse = serde_json::from_str(&primary_text)?;
        let primary_choice = primary_parsed
            .choices
            .first()
            .ok_or_else(|| anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆä¸»æ¬„ä½"))?;

        let primary_task: AITaskPrimaryFields = serde_json::from_str(&primary_choice.message.content)?;

        let secondary_prompt = format!(
            r#"åŸºæ–¼ä»¥ä¸‹ä»»å‹™ä¸»è¦æ¬„ä½è³‡è¨Šï¼Œè£œå…¨å‰©é¤˜æ¬„ä½ã€‚

**ä»»å‹™ä¸»è¦æ¬„ä½ï¼š**
{}

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{{
  "priority": 0-2,
  "difficulty": 1-5,
  "experience": ç¶“é©—å€¼,
  "is_recurring": å¸ƒæ—å€¼,
  "completion_target": å®Œæˆç‡ç›®æ¨™ï¼ˆé‡è¤‡æ€§ä»»å‹™æ™‚æä¾›ï¼Œå¦å‰‡ç‚º nullï¼‰ï¼Œ
  "start_date": "é–‹å§‹æ—¥æœŸï¼ˆISO 8601æ ¼å¼ï¼Œåƒ…åœ¨éœ€è¦æ™‚æä¾›ï¼‰",
  "end_date": "çµæŸæ—¥æœŸï¼ˆISO 8601æ ¼å¼ï¼Œåƒ…åœ¨éœ€è¦æ™‚æä¾›ï¼‰"
}}

è¦å‰‡ï¼š
- å„ªå…ˆç´šï¼š0=ä½, 1=ä¸­, 2=é«˜ã€‚
- é›£åº¦ï¼š1=éå¸¸ç°¡å–®, 5=éå¸¸å›°é›£ã€‚
- ç¶“é©—å€¼é€šå¸¸æ˜¯ difficulty * 20 + priority * 10ã€‚
- è‹¥ä»»å‹™ç‚ºé‡è¤‡æ€§ï¼Œis_recurring æ‡‰ç‚º trueï¼Œcompletion_target é è¨­ 0.8ï¼Œstart_date éœ€æä¾›ï¼Œdue_date ä¿æŒç‚º nullã€‚
- è‹¥éé‡è¤‡æ€§ä»»å‹™ï¼Œis_recurring ç‚º falseï¼Œcompletion_targetã€start_dateã€end_date é è¨­ç‚º nullã€‚
"#,
            serde_json::to_string_pretty(&primary_task)?
        );

        let secondary_request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: secondary_prompt,
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: "è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šè£œå…¨å‰©é¤˜æ¬„ä½".to_string(),
                },
            ],
            max_completion_tokens: 2000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&secondary_request) {
            log::info!("[AI INPUT][generate_task_from_text_secondary] {}", format_ai_output(&body));
        }

        let secondary_response = self
            .client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&secondary_request)
            .send()
            .await?;

        let secondary_status = secondary_response.status();
        let secondary_text = secondary_response.text().await?;
        log::info!("[AI OUTPUT][generate_task_from_text_secondary] {}", format_ai_output(&secondary_text));

        if !secondary_status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ (secondary) ({}): {}", secondary_status, secondary_text));
        }

        let secondary_parsed: OpenAIResponse = serde_json::from_str(&secondary_text)?;
        let secondary_choice = secondary_parsed
            .choices
            .first()
            .ok_or_else(|| anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆæ¬¡æ¬„ä½"))?;

        let secondary_task: AITaskSecondaryFields = serde_json::from_str(&secondary_choice.message.content)?;

        let combined_task = AIGeneratedTask {
            title: primary_task.title,
            description: primary_task.description,
            task_type: primary_task.task_type,
            priority: secondary_task.priority,
            difficulty: secondary_task.difficulty,
            experience: secondary_task.experience,
            due_date: primary_task.due_date,
            is_recurring: secondary_task.is_recurring,
            recurrence_pattern: primary_task.recurrence_pattern,
            start_date: secondary_task.start_date,
            end_date: secondary_task.end_date,
            completion_target: secondary_task.completion_target,
        }
        .with_defaults()
        .normalize_recurring();

        let validated_task = validate_generated_task(&combined_task)?;

        Ok(validated_task)
    }

    async fn match_expert_for_task(&self, user_input: &str) -> Result<ExpertMatch> {
        let experts = get_expert_database();

        // æ§‹å»ºå°ˆå®¶åŒ¹é…çš„æç¤ºè©
        let expert_list = experts.iter()
            .enumerate()
            .map(|(i, expert)| {
                format!("{}. {} ({}) - å°ˆç²¾é ˜åŸŸ: {}",
                    i + 1,
                    expert.name,
                    expert.emoji,
                    expert.expertise_areas.join("ã€")
                )
            })
            .collect::<Vec<_>>()
            .join("\n");

        let system_prompt = format!(
            r#"ä½ æ˜¯ä¸€å€‹å°ˆå®¶åŒ¹é…åŠ©æ‰‹ã€‚æ ¹æ“šç”¨æˆ¶çš„ä»»å‹™æè¿°ï¼Œå¾ä»¥ä¸‹å°ˆå®¶åˆ—è¡¨ä¸­é¸æ“‡æœ€é©åˆçš„å°ˆå®¶ã€‚

å¯ç”¨å°ˆå®¶åˆ—è¡¨ï¼š
{}

è«‹åˆ†æç”¨æˆ¶çš„ä»»å‹™æè¿°ï¼Œé¸æ“‡æœ€é©åˆçš„å°ˆå®¶ï¼Œä¸¦æä¾›åŒ¹é…ç†ç”±ã€‚

å›æ‡‰æ ¼å¼ï¼ˆJSONï¼‰ï¼š
{{
  "expert_name": "å°ˆå®¶çš„å®Œæ•´åç¨±",
  "expert_description": "å°ˆå®¶çš„è©³ç´°æè¿°"
}}

é¸æ“‡åŸå‰‡ï¼š
1. æ ¹æ“šä»»å‹™çš„æ ¸å¿ƒé ˜åŸŸé¸æ“‡å°ˆå®¶
2. è€ƒæ…®å°ˆå®¶çš„å°ˆæ¥­é ˜åŸŸæ˜¯å¦èˆ‡ä»»å‹™åŒ¹é…
3. å¦‚æœæ²’æœ‰å®Œå…¨åŒ¹é…çš„å°ˆå®¶ï¼Œé¸æ“‡æœ€æ¥è¿‘çš„"#,
            expert_list
        );

        log::info!("[AI INPUT][match_expert_for_task] {}", format_ai_output(&user_input));

        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: system_prompt,
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: user_input.to_string(),
                },
            ],
            max_completion_tokens: 4000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!("[AI INPUT][match_expert_for_task_payload] {}", format_ai_output(&body));
        }

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!("[AI OUTPUT][match_expert_for_task] {}", format_ai_output(&response_text));

        if !status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ ({}): {}", status, response_text));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;

        if let Some(choice) = openai_response.choices.first() {
            let match_json = &choice.message.content;
            let match_result: serde_json::Value = serde_json::from_str(match_json)?;

            let expert_name = match_result["expert_name"].as_str()
                .ok_or_else(|| anyhow::anyhow!("ç„¡æ•ˆçš„å°ˆå®¶åç¨±"))?.to_string();

            let expert_description = match_result["expert_description"].as_str()
                .ok_or_else(|| anyhow::anyhow!("ç„¡æ•ˆçš„å°ˆå®¶æè¿°"))?.to_string();

            // ç›´æ¥ä½¿ç”¨AIè¿”å›çš„å°ˆå®¶ä¿¡æ¯ï¼Œå‰µå»ºè™›æ“¬å°ˆå®¶å°è±¡
            let virtual_expert = Expert {
                name: expert_name.clone(),
                description: expert_description.clone(),
                expertise_areas: vec!["AIåŒ¹é…".to_string()],
                emoji: "ğŸ¤–".to_string(),
            };

            Ok(ExpertMatch {
                expert: virtual_expert,
                ai_expert_name: expert_name,
                ai_expert_description: expert_description,
            })
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }

    async fn generate_task_with_expert(&self, user_input: &str, expert_match: &ExpertMatch) -> Result<AIGeneratedTaskPlan> {
        let now = Utc::now();
        let current_time_str = now.format("%Y-%m-%dT%H:%M:%S").to_string();

        let system_prompt = format!(
            r#"ä½ æ˜¯{}ï¼Œ{}

**é‡è¦ï¼šç¾åœ¨çš„æ™‚é–“æ˜¯ {}ã€‚** åœ¨ç”Ÿæˆä»»ä½•èˆ‡æ—¥æœŸç›¸é—œçš„æ¬„ä½ï¼ˆå¦‚ due_dateï¼‰æ™‚ï¼Œè«‹ä»¥æ­¤æ™‚é–“ç‚ºåŸºæº–é€²è¡Œæ¨ç®—ã€‚

è«‹æ ¹æ“šç”¨æˆ¶éœ€æ±‚ç”Ÿæˆä¸€å€‹å®Œæ•´çš„å­¸ç¿’ä»»å‹™ã€‚

è¦æ±‚ï¼š
1. ä¸»ä»»å‹™ä½œç‚ºæ•´é«”å­¸ç¿’ç›®æ¨™ï¼Œtask_type å¿…é ˆç‚º "main"
2. ä»»å‹™æè¿°æ‡‰è©²è©³ç´°ä¸”å…·é«”ï¼ŒåŒ…å«å­¸ç¿’ç›®æ¨™ã€æ–¹æ³•å»ºè­°ç­‰
3. å­¸ç¿’å‹ä»»å‹™ä¸è¨­ç‚ºé‡è¤‡æ€§ï¼Œis_recurring å¿…é ˆç‚º falseï¼Œrecurrence_pattern å¿…é ˆç‚º null
4. ä¸»ä»»å‹™å›ºå®šè¨­ç½®ï¼špriority = 2ã€difficulty = 3ã€experience = 100
5. ä¸éœ€è¦è¨­ç½® start_dateã€end_dateã€completion_targetï¼ˆå…¨éƒ¨ç‚º nullï¼‰

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼ŒåŒ…å«ä»¥ä¸‹æ‰€æœ‰æ¬„ä½ï¼š
{{
  "title": "ä»»å‹™æ¨™é¡Œ",
  "description": "è©³ç´°æè¿°ï¼ˆåŒ…å«å­¸ç¿’ç›®æ¨™å’Œæ–¹æ³•å»ºè­°ï¼‰",
  "task_type": "main",
  "priority": 2,
  "difficulty": 3,
  "experience": 100,
  "due_date": "ISO 8601 æ ¼å¼æ™‚é–“æˆ– null",
  "is_recurring": false,
  "recurrence_pattern": null,
  "start_date": null,
  "end_date": null,
  "completion_target": null
}}

ä¸è¦è¼¸å‡ºå…¶ä»–æ¬„ä½æˆ–é¡å¤–æ–‡å­—ã€‚"#,
            expert_match.ai_expert_name,
            expert_match.ai_expert_description,
            current_time_str
        );

        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: system_prompt,
                },
                ChatMessage {
                    role: "user".to_string(),
                    content: format!("è«‹æ ¹æ“šä»¥ä¸‹æè¿°ç”Ÿæˆå®Œæ•´çš„å­¸ç¿’ä»»å‹™ï¼š{}", user_input),
                },
            ],
            max_completion_tokens: 3000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!(
                "[AI INPUT][generate_task_with_expert][OpenAI] {}",
                format_ai_output(&body)
            );
        }

        let response = self
            .client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!(
            "[AI OUTPUT][generate_task_with_expert][OpenAI] {}",
            format_ai_output(&response_text)
        );

        if !status.is_success() {
            return Err(anyhow::anyhow!(
                "OpenAI API éŒ¯èª¤ ({}): {}",
                status,
                response_text
            ));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;
        let choice = openai_response
            .choices
            .first()
            .ok_or_else(|| anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))?;

        // ç›´æ¥è§£æç‚º AIGeneratedTask
        let mut main_task: AIGeneratedTask = serde_json::from_str(&choice.message.content)?;

        // ç¢ºä¿è¨­ç½®æ­£ç¢ºçš„é»˜èªå€¼
        main_task.task_type = Some("main".to_string());
        main_task.priority = Some(2);
        main_task.difficulty = Some(3);
        main_task.experience = Some(100);
        main_task.is_recurring = Some(false);
        main_task.recurrence_pattern = None;
        main_task.start_date = None;
        main_task.end_date = None;
        main_task.completion_target = None;

        let main_task = main_task.with_defaults().normalize_recurring();
        let validated_main_task = validate_generated_task(&main_task)?;

        // ä¸ç”Ÿæˆå­ä»»å‹™
        let subtasks: Vec<AIGeneratedTask> = Vec::new();

        Ok(AIGeneratedTaskPlan {
            main_task: validated_main_task,
            subtasks,
        })
    }

    async fn analyze_with_expert(&self, user_input: &str, expert_name: &str, expert_description: &str, analysis_type: &str) -> Result<String> {
        let analysis_prompts = match analysis_type {
            "analyze" => format!(
                r#"ä½ æ˜¯{}ï¼Œ{}

è«‹æ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚åˆ†æå‡º5å€‹é©åˆçš„åŠ å¼·æ–¹å‘ã€‚
ç”¨æˆ¶éœ€æ±‚ï¼š{}
æ¯å€‹æ–¹å‘æ¨™é¡Œè¦ç°¡æ½”æ˜ç¢ºï¼Œæè¿°è¦ç°¡çŸ­ï¼ˆä¸è¶…é20å­—ï¼‰ã€‚
è«‹ä»¥JSONæ ¼å¼å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "directions": [
    {{"title": "æ–¹å‘æ¨™é¡Œ", "description": "ç°¡çŸ­æè¿°"}},
    {{"title": "æ–¹å‘æ¨™é¡Œ", "description": "ç°¡çŸ­æè¿°"}}
  ]
}}
"#,
                expert_name, expert_description, user_input
            ),
            "goals" => format!(
                r#"ä½ æ˜¯{}ï¼Œ{}
è«‹æ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚ç”Ÿæˆ5å€‹æ˜ç¢ºã€å¯è¡¡é‡çš„å­¸ç¿’ç›®æ¨™ã€‚ç›®æ¨™æ‡‰è©²å…·é«”ã€å¯é”æˆã€æœ‰æ™‚é–“æ€§ã€‚
æ¯å€‹ç›®æ¨™æ¨™é¡Œè¦ç°¡æ½”æ˜ç¢ºï¼Œæè¿°è¦åŒ…å«å…·é«”çš„è¡¡é‡æ¨™æº–ï¼ˆä¸è¶…é30å­—ï¼‰ã€‚
ç”¨æˆ¶éœ€æ±‚ï¼š{}
è«‹ä»¥JSONæ ¼å¼å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "goals": [
    {{"title": "ç›®æ¨™æ¨™é¡Œ", "description": "å…·é«”æè¿°å’Œè¡¡é‡æ¨™æº–"}},
    {{"title": "ç›®æ¨™æ¨™é¡Œ", "description": "å…·é«”æè¿°å’Œè¡¡é‡æ¨™æº–"}},
    {{"title": "ç›®æ¨™æ¨™é¡Œ", "description": "å…·é«”æè¿°å’Œè¡¡é‡æ¨™æº–"}},
    {{"title": "ç›®æ¨™æ¨™é¡Œ", "description": "å…·é«”æè¿°å’Œè¡¡é‡æ¨™æº–"}},
    {{"title": "ç›®æ¨™æ¨™é¡Œ", "description": "å…·é«”æè¿°å’Œè¡¡é‡æ¨™æº–"}}
  ]
}}

å¿…é ˆè¿”å›æ°å¥½5å€‹ç›®æ¨™ã€‚"#,
                expert_name, expert_description, user_input
            ),
            "resources" => format!(
                r#"ä½ æ˜¯{}ï¼Œ{}

è«‹æ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚æ¨è–¦5å€‹å„ªè³ªçš„å­¸ç¿’è³‡æºï¼ŒåŒ…æ‹¬æ›¸ç±ã€èª²ç¨‹ã€ç¶²ç«™ã€å·¥å…·ç­‰ã€‚

ç”¨æˆ¶éœ€æ±‚ï¼š{}

è«‹ä»¥JSONæ ¼å¼å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "resources": [
    {{"title": "è³‡æºåç¨±", "description": "è³‡æºæè¿°å’Œæ¨è–¦ç†ç”±"}},
    {{"title": "è³‡æºåç¨±", "description": "è³‡æºæè¿°å’Œæ¨è–¦ç†ç”±"}},
    {{"title": "è³‡æºåç¨±", "description": "è³‡æºæè¿°å’Œæ¨è–¦ç†ç”±"}},
    {{"title": "è³‡æºåç¨±", "description": "è³‡æºæè¿°å’Œæ¨è–¦ç†ç”±"}},
    {{"title": "è³‡æºåç¨±", "description": "è³‡æºæè¿°å’Œæ¨è–¦ç†ç”±"}}
  ]
}}

å¿…é ˆè¿”å›æ°å¥½5å€‹å­¸ç¿’è³‡æºã€‚æ¯å€‹è³‡æºåç¨±è¦ç°¡æ½”æ˜ç¢ºï¼Œæè¿°è¦ç°¡çŸ­èªªæ˜ç‚ºä»€éº¼æ¨è–¦ï¼ˆä¸è¶…é30å­—ï¼‰ã€‚"#,
                expert_name, expert_description, user_input
            ),
            _ => return Err(anyhow::anyhow!("ä¸æ”¯æ´çš„åˆ†æé¡å‹: {}", analysis_type)),
        };

        log::info!("[AI INPUT][analyze_with_expert] description={} type={} expert_name={} expert_description={}", user_input, analysis_type, expert_name, expert_description);

        let request = OpenAIRequest {
            model: self.model.clone(),
            messages: vec![
                ChatMessage {
                    role: "system".to_string(),
                    content: analysis_prompts,
                },
            ],
            max_completion_tokens: 4000,
            response_format: ResponseFormat {
                format_type: "json_object".to_string(),
            },
        };

        if let Ok(body) = serde_json::to_string(&request) {
            log::info!("[AI INPUT][analyze_with_expert_payload] {}", format_ai_output(&body));
        }

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        let status = response.status();
        let response_text = response.text().await?;
        log::info!("[AI OUTPUT][analyze_with_expert] {}", format_ai_output(&response_text));

        if !status.is_success() {
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤ ({}): {}", status, response_text));
        }

        let openai_response: OpenAIResponse = serde_json::from_str(&response_text)?;

        if let Some(choice) = openai_response.choices.first() {
            Ok(choice.message.content.clone())
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }

    async fn generate_subtasks_for_main_task(&self, _main_task_title: &str, _main_task_description: &str, _expert_match: &ExpertMatch) -> Result<Vec<AIGeneratedTask>> {
        // OpenAIæœå‹™æš«æ™‚ä¸æ”¯æ´ï¼Œè¿”å›ç©ºåˆ—è¡¨
        log::warn!("OpenAIæœå‹™æš«æ™‚ä¸æ”¯æ´ç”Ÿæˆå­ä»»å‹™");
        Ok(Vec::new())
    }

    async fn generate_with_model(&self, model: &str, prompt: &str) -> Result<String> {
        // æ ¹æ“šæ¨¡å‹é¡å‹å‹•æ…‹èª¿æ•´ max_tokens
        let max_tokens = if model.contains("perplexity") {
            16000  // Perplexity æ¨¡å‹çµ¦äºˆæ›´å¤§çš„ç©ºé–“
        } else if model.contains("claude") || model.contains("anthropic") {
            8000   // Claude æ¨¡å‹éœ€è¦æ›´å¤šç©ºé–“ä¾†ç”Ÿæˆå®Œæ•´çš„ä»»å‹™ç´°ç¯€
        } else {
            4000   // å…¶ä»–æ¨¡å‹ä½¿ç”¨é è¨­å€¼
        };

        log::info!("ä½¿ç”¨æ¨¡å‹ {} ç”Ÿæˆå›æ‡‰ï¼Œmax_completion_tokens: {}", model, max_tokens);

        let request = serde_json::json!({
            "model": model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_completion_tokens": max_tokens
        });

        let response = self.client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(anyhow::anyhow!("OpenAI API éŒ¯èª¤: {}", error_text));
        }

        let openai_response: OpenAIResponse = response.json().await?;

        if let Some(choice) = openai_response.choices.first() {
            Ok(choice.message.content.clone())
        } else {
            Err(anyhow::anyhow!("OpenAI æœªè¿”å›æœ‰æ•ˆå›æ‡‰"))
        }
    }
}